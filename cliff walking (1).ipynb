{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04b291d0",
   "metadata": {},
   "source": [
    "# 3.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f667e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a06838a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "gamma = 1  # Discount factor\n",
    "alpha = 0.1  # Learning rate\n",
    "iterations = 500\n",
    "col_size = 21\n",
    "row_size = 4\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4618d99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid and states\n",
    "cliff = [(3, i) for i in range(1, col_size - 1)]\n",
    "start = (3, 0)\n",
    "goal = (3, 20)\n",
    "states = [(i, j) for i in range(row_size) for j in range(col_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb40c73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actions\n",
    "actions = [(-1, 0), (1, 0), (0, 1), (0, -1)]\n",
    "\n",
    "def transition(state, action):\n",
    "    new_state = (max(min(state[0] + action[0], row_size - 1), 0),\n",
    "                 max(min(state[1] + action[1], col_size - 1), 0))\n",
    "    return new_state if new_state != state else state\n",
    "\n",
    "def take_action(state, action):\n",
    "    next_state = transition(state, action)\n",
    "    reward = 20 if next_state == goal else -100 if next_state in cliff else -1\n",
    "    terminal = next_state == goal or next_state in cliff\n",
    "    return next_state, reward, terminal\n",
    "\n",
    "def choose_action(state, qvalues, epsilon):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return random.choice(actions)\n",
    "    else:\n",
    "        return max(actions, key=lambda a: qvalues[state][a])\n",
    "\n",
    "def show_route(visited_states):\n",
    "    for i in range(row_size):\n",
    "        print('-' * (col_size * 4))\n",
    "        for j in range(col_size):\n",
    "            token = 'R' if (i, j) in visited_states else 'G' if (i, j) == goal else '*' if (i, j) in cliff else '0'\n",
    "            print(f'| {token} ', end='')\n",
    "        print('|')\n",
    "    print('-' * (col_size * 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2bebe34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def showRoute(states):\n",
    "    board = np.zeros([row_size, col_size])\n",
    "    # Add cliff marked as -1\n",
    "    for c in cliff:\n",
    "        board[c[0], c[1]] = -1\n",
    "\n",
    "    for state in states:\n",
    "        if state != start and state != goal:  # Avoid overwriting start and goal\n",
    "            board[state[0], state[1]] = 1  # Mark the route\n",
    "\n",
    "    for i in range(row_size):\n",
    "        print('--------------------------------------------')\n",
    "        out = ' '\n",
    "        for j in range(col_size):\n",
    "            token = ' '\n",
    "            if board[i, j] == -1:\n",
    "                token = 'C'\n",
    "            elif board[i, j] == 1:\n",
    "                token = '1'\n",
    "            if (i, j) == start:\n",
    "                token = 'S'\n",
    "            if (i, j) == goal:\n",
    "                token = 'G'\n",
    "            out += token + ' '\n",
    "        print(out)\n",
    "    print('--------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6504f76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(episodes, epsilon, start, alpha, qvalues=None):\n",
    "    if qvalues is None:\n",
    "        qvalues = {(i, j): {tuple(a): 0.0 for a in actions}  # Convert actions to tuples\n",
    "                   for i in range(row_size) for j in range(col_size)}\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        current_state = start\n",
    "        current_action = choose_action(current_state, qvalues, epsilon)\n",
    "        terminal = False\n",
    "\n",
    "        while not terminal:\n",
    "            next_state, reward, terminal = take_action(current_state, current_action)\n",
    "            next_action = choose_action(next_state, qvalues, epsilon) if not terminal else None\n",
    "\n",
    "            # SARSA update\n",
    "            current_value = qvalues[current_state][current_action]\n",
    "            next_value = qvalues[next_state][next_action] if next_action is not None else 0\n",
    "            qvalues[current_state][current_action] += alpha * (reward + gamma * next_value - current_value)\n",
    "\n",
    "            current_state, current_action = next_state, next_action\n",
    "\n",
    "    return qvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0f5d360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qlearning(episodes, epsilon, start, alpha, gamma, qvalues=None):\n",
    "    if qvalues is None:\n",
    "        qvalues = {(i, j): {a: 0.0 for a in actions}\n",
    "                   for i in range(row_size) for j in range(col_size)}\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        current_state = start\n",
    "        terminal = False\n",
    "\n",
    "        while not terminal:\n",
    "            current_action = choose_action(current_state, qvalues, epsilon)\n",
    "            next_state, reward, terminal = take_action(current_state, current_action)\n",
    "\n",
    "            # Q-Learning update\n",
    "            current_value = qvalues[current_state][current_action]\n",
    "            next_value = max(qvalues[next_state].values()) if not terminal else 0\n",
    "            qvalues[current_state][current_action] += alpha * (reward + gamma * next_value - current_value)\n",
    "\n",
    "            current_state = next_state\n",
    "\n",
    "    return qvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1048d64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_policy_path(start, qvalues):\n",
    "    current_state = start\n",
    "    policy_path = [current_state]\n",
    "    terminal = False\n",
    "\n",
    "    while not terminal:\n",
    "        current_action = max(qvalues[current_state], key=qvalues[current_state].get)\n",
    "        next_state, _, terminal = take_action(current_state, current_action)\n",
    "        policy_path.append(next_state)\n",
    "        current_state = next_state\n",
    "        if current_state == goal:  \n",
    "            break\n",
    "\n",
    "    return policy_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10d4a915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 0), (2, 0), (1, 0), (1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6), (1, 7), (1, 8), (1, 9), (1, 10), (1, 11), (1, 12), (1, 13), (1, 14), (1, 15), (1, 16), (1, 17), (1, 18), (1, 19), (1, 20), (2, 20), (3, 20)]\n",
      "--------------------------------------------\n",
      "                                           \n",
      "--------------------------------------------\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 \n",
      "--------------------------------------------\n",
      " 1                                       1 \n",
      "--------------------------------------------\n",
      " S C C C C C C C C C C C C C C C C C C C G \n",
      "--------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Run SARSA to get Q-values\n",
    "qs_s = sarsa(1000, epsilon, start, alpha)\n",
    "\n",
    "# Generate policy path from the learned Q-values\n",
    "states_s = generate_policy_path(start, qs_s)\n",
    "\n",
    "print(states_s)\n",
    "showRoute(states_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7fcd7aa4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 0), (2, 0), (2, 1), (2, 2), (2, 3), (2, 4), (2, 5), (2, 6), (2, 7), (2, 8), (2, 9), (2, 10), (2, 11), (2, 12), (2, 13), (2, 14), (2, 15), (2, 16), (2, 17), (2, 18), (2, 19), (2, 20), (3, 20)]\n",
      "--------------------------------------------\n",
      "                                           \n",
      "--------------------------------------------\n",
      "                                           \n",
      "--------------------------------------------\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 \n",
      "--------------------------------------------\n",
      " S C C C C C C C C C C C C C C C C C C C G \n",
      "--------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Run Q-learning to get Q-values\n",
    "qs_q = qlearning(1000, epsilon, start, alpha, gamma)\n",
    "\n",
    "# Generate policy path from the learned Q-values\n",
    "states_q = generate_policy_path(start, qs_q)\n",
    "\n",
    "print(states_q)\n",
    "showRoute(states_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec0d603",
   "metadata": {},
   "source": [
    "# 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92155c60",
   "metadata": {},
   "source": [
    "## Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b457b747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e = 0\n",
      "[(3, 0), (2, 0), (2, 1), (2, 2), (2, 3), (2, 4), (2, 5), (2, 6), (2, 7), (2, 8), (2, 9), (2, 10), (2, 11), (2, 12), (2, 13), (2, 14), (2, 15), (2, 16), (2, 17), (2, 18), (2, 19), (2, 20), (3, 20)]\n",
      "--------------------------------------------\n",
      "                                           \n",
      "--------------------------------------------\n",
      "                                           \n",
      "--------------------------------------------\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 \n",
      "--------------------------------------------\n",
      " S C C C C C C C C C C C C C C C C C C C G \n",
      "--------------------------------------------\n",
      "e = 0.2\n",
      "[(3, 0), (2, 0), (1, 0), (0, 0), (0, 1), (0, 2), (0, 3), (0, 4), (0, 5), (0, 6), (0, 7), (0, 8), (0, 9), (0, 10), (0, 11), (0, 12), (0, 13), (1, 13), (1, 14), (1, 15), (1, 16), (1, 17), (1, 18), (1, 19), (1, 20), (2, 20), (3, 20)]\n",
      "--------------------------------------------\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1               \n",
      "--------------------------------------------\n",
      " 1                         1 1 1 1 1 1 1 1 \n",
      "--------------------------------------------\n",
      " 1                                       1 \n",
      "--------------------------------------------\n",
      " S C C C C C C C C C C C C C C C C C C C G \n",
      "--------------------------------------------\n",
      "e = 0.5\n",
      "[(3, 0), (2, 0), (1, 0), (0, 0), (0, 1), (0, 2), (0, 3), (0, 4), (0, 5), (0, 6), (0, 7), (0, 8), (0, 9), (0, 10), (0, 11), (0, 12), (0, 13), (0, 14), (0, 15), (0, 16), (0, 17), (0, 18), (0, 19), (0, 20), (1, 20), (2, 20), (3, 20)]\n",
      "--------------------------------------------\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 \n",
      "--------------------------------------------\n",
      " 1                                       1 \n",
      "--------------------------------------------\n",
      " 1                                       1 \n",
      "--------------------------------------------\n",
      " S C C C C C C C C C C C C C C C C C C C G \n",
      "--------------------------------------------\n",
      "e = 0.8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m qs_q \u001b[38;5;241m=\u001b[39m sarsa(\u001b[38;5;241m1000\u001b[39m, epsilon, start, alpha)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Generate policy path from the learned Q-values\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m states_q \u001b[38;5;241m=\u001b[39m generate_policy_path(start, qs_q)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(states_q)\n\u001b[0;32m     43\u001b[0m showRoute(states_q)\n",
      "Cell \u001b[1;32mIn[10], line 8\u001b[0m, in \u001b[0;36mgenerate_policy_path\u001b[1;34m(start, qvalues)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m terminal:\n\u001b[0;32m      7\u001b[0m     current_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(qvalues[current_state], key\u001b[38;5;241m=\u001b[39mqvalues[current_state]\u001b[38;5;241m.\u001b[39mget)\n\u001b[1;32m----> 8\u001b[0m     next_state, _, terminal \u001b[38;5;241m=\u001b[39m take_action(current_state, current_action)\n\u001b[0;32m      9\u001b[0m     policy_path\u001b[38;5;241m.\u001b[39mappend(next_state)\n\u001b[0;32m     10\u001b[0m     current_state \u001b[38;5;241m=\u001b[39m next_state\n",
      "Cell \u001b[1;32mIn[6], line 9\u001b[0m, in \u001b[0;36mtake_action\u001b[1;34m(state, action)\u001b[0m\n\u001b[0;32m      5\u001b[0m     new_state \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mmin\u001b[39m(state[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m action[\u001b[38;5;241m0\u001b[39m], row_size \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m0\u001b[39m),\n\u001b[0;32m      6\u001b[0m                  \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mmin\u001b[39m(state[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m action[\u001b[38;5;241m1\u001b[39m], col_size \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_state \u001b[38;5;28;01mif\u001b[39;00m new_state \u001b[38;5;241m!=\u001b[39m state \u001b[38;5;28;01melse\u001b[39;00m state\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtake_action\u001b[39m(state, action):\n\u001b[0;32m     10\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m transition(state, action)\n\u001b[0;32m     11\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m next_state \u001b[38;5;241m==\u001b[39m goal \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m next_state \u001b[38;5;129;01min\u001b[39;00m cliff \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Test different epsilon value for SARSA\n",
    "iterations = 500\n",
    "\n",
    "print(\"e = 0\")\n",
    "epsilon = 0\n",
    "qs_q = sarsa(1000, epsilon, start, alpha)\n",
    "\n",
    "# Generate policy path from the learned Q-values\n",
    "states_q = generate_policy_path(start, qs_q)\n",
    "\n",
    "print(states_q)\n",
    "showRoute(states_q)\n",
    "\n",
    "print(\"e = 0.2\")\n",
    "epsilon = 0.2\n",
    "qs_q = sarsa(1000, epsilon, start, alpha)\n",
    "\n",
    "# Generate policy path from the learned Q-values\n",
    "states_q = generate_policy_path(start, qs_q)\n",
    "\n",
    "print(states_q)\n",
    "showRoute(states_q)\n",
    "\n",
    "print(\"e = 0.5\")\n",
    "epsilon = 0.5\n",
    "qs_q = sarsa(1000, epsilon, start, alpha)\n",
    "\n",
    "# Generate policy path from the learned Q-values\n",
    "states_q = generate_policy_path(start, qs_q)\n",
    "\n",
    "print(states_q)\n",
    "showRoute(states_q)\n",
    "\n",
    "\n",
    "print(\"e = 0.8\")\n",
    "epsilon = 0.8\n",
    "qs_q = sarsa(1000, epsilon, start, alpha)\n",
    "\n",
    "# Generate policy path from the learned Q-values\n",
    "states_q = generate_policy_path(start, qs_q)\n",
    "\n",
    "print(states_q)\n",
    "showRoute(states_q)\n",
    "\n",
    "print(\"e = 1\")\n",
    "epsilon = 1\n",
    "qs_q = sarsa(1000, epsilon, start, alpha)\n",
    "\n",
    "# Generate policy path from the learned Q-values\n",
    "states_q = generate_policy_path(start, qs_q)\n",
    "\n",
    "print(states_q)\n",
    "showRoute(states_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2332cf8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e = 0\n",
      "[(3, 0), (2, 0), (2, 1), (2, 2), (2, 3), (2, 4), (2, 5), (2, 6), (2, 7), (2, 8), (2, 9), (2, 10), (2, 11), (2, 12), (2, 13), (2, 14), (2, 15), (2, 16), (2, 17), (2, 18), (2, 19), (2, 20), (3, 20)]\n",
      "--------------------------------------------\n",
      "                                           \n",
      "--------------------------------------------\n",
      "                                           \n",
      "--------------------------------------------\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 \n",
      "--------------------------------------------\n",
      " S C C C C C C C C C C C C C C C C C C C G \n",
      "--------------------------------------------\n",
      "e = 0.2\n",
      "[(3, 0), (2, 0), (2, 1), (2, 2), (2, 3), (2, 4), (2, 5), (2, 6), (2, 7), (2, 8), (2, 9), (2, 10), (2, 11), (2, 12), (2, 13), (2, 14), (2, 15), (2, 16), (2, 17), (2, 18), (2, 19), (2, 20), (3, 20)]\n",
      "--------------------------------------------\n",
      "                                           \n",
      "--------------------------------------------\n",
      "                                           \n",
      "--------------------------------------------\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 \n",
      "--------------------------------------------\n",
      " S C C C C C C C C C C C C C C C C C C C G \n",
      "--------------------------------------------\n",
      "e = 0.5\n",
      "[(3, 0), (2, 0), (2, 1), (2, 2), (2, 3), (2, 4), (2, 5), (2, 6), (2, 7), (2, 8), (2, 9), (2, 10), (2, 11), (2, 12), (2, 13), (2, 14), (2, 15), (2, 16), (2, 17), (2, 18), (2, 19), (2, 20), (3, 20)]\n",
      "--------------------------------------------\n",
      "                                           \n",
      "--------------------------------------------\n",
      "                                           \n",
      "--------------------------------------------\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 \n",
      "--------------------------------------------\n",
      " S C C C C C C C C C C C C C C C C C C C G \n",
      "--------------------------------------------\n",
      "e = 0.8\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m qs_q \u001b[38;5;241m=\u001b[39m qlearning(\u001b[38;5;241m1000\u001b[39m, epsilon, start, alpha, gamma)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Generate policy path from the learned Q-values\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m states_q \u001b[38;5;241m=\u001b[39m generate_policy_path(start, qs_q)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(states_q)\n\u001b[0;32m     42\u001b[0m showRoute(states_q)\n",
      "Cell \u001b[1;32mIn[10], line 9\u001b[0m, in \u001b[0;36mgenerate_policy_path\u001b[1;34m(start, qvalues)\u001b[0m\n\u001b[0;32m      7\u001b[0m current_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(qvalues[current_state], key\u001b[38;5;241m=\u001b[39mqvalues[current_state]\u001b[38;5;241m.\u001b[39mget)\n\u001b[0;32m      8\u001b[0m next_state, _, terminal \u001b[38;5;241m=\u001b[39m take_action(current_state, current_action)\n\u001b[1;32m----> 9\u001b[0m policy_path\u001b[38;5;241m.\u001b[39mappend(next_state)\n\u001b[0;32m     10\u001b[0m current_state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m current_state \u001b[38;5;241m==\u001b[39m goal:  \n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Test different epsilon value for Q-Learning\n",
    "iterations = 500\n",
    "\n",
    "print(\"e = 0\")\n",
    "epsilon = 0\n",
    "qs_q = qlearning(1000, epsilon, start, alpha, gamma)\n",
    "\n",
    "# Generate policy path from the learned Q-values\n",
    "states_q = generate_policy_path(start, qs_q)\n",
    "\n",
    "print(states_q)\n",
    "showRoute(states_q)\n",
    "\n",
    "print(\"e = 0.2\")\n",
    "epsilon = 0.2\n",
    "qs_q = qlearning(1000, epsilon, start, alpha, gamma)\n",
    "\n",
    "# Generate policy path from the learned Q-values\n",
    "states_q = generate_policy_path(start, qs_q)\n",
    "\n",
    "print(states_q)\n",
    "showRoute(states_q)\n",
    "\n",
    "print(\"e = 0.5\")\n",
    "epsilon = 0.5\n",
    "qs_q = qlearning(1000, epsilon, start, alpha, gamma)\n",
    "\n",
    "# Generate policy path from the learned Q-values\n",
    "states_q = generate_policy_path(start, qs_q)\n",
    "\n",
    "print(states_q)\n",
    "showRoute(states_q)\n",
    "\n",
    "print(\"e = 0.8\")\n",
    "epsilon = 0.8\n",
    "qs_q = qlearning(1000, epsilon, start, alpha, gamma)\n",
    "\n",
    "# Generate policy path from the learned Q-values\n",
    "states_q = generate_policy_path(start, qs_q)\n",
    "\n",
    "print(states_q)\n",
    "showRoute(states_q)\n",
    "\n",
    "print(\"e = 1\")\n",
    "epsilon = 1\n",
    "qs_q = qlearning(1000, epsilon, start, alpha, gamma)\n",
    "\n",
    "# Generate policy path from the learned Q-values\n",
    "states_q = generate_policy_path(start, qs_q)\n",
    "\n",
    "print(states_q)\n",
    "showRoute(states_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc178ef",
   "metadata": {},
   "source": [
    "## Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e5e264",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cec55c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898218a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10b86d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7e8458",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cc5c92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43b117ca",
   "metadata": {},
   "source": [
    "# 3.3 Snake Pits Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "19fed883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "gamma = 1  # Discount factor\n",
    "alpha = 0.1  # Learning rate\n",
    "iterations = 500\n",
    "col_size = 21\n",
    "row_size = 4\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5e05526b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid and states\n",
    "cliff = [(3, i) for i in range(1, col_size - 1)]  \n",
    "snake_pit = [(1, 10), (2, 10)]  # Snake pit locations in the two bottom cells of the 11th column\n",
    "start = (3, 0)  \n",
    "goal = (3, col_size - 1)  \n",
    "states = [(i, j) for i in range(row_size) for j in range(col_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c1c9d07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actions\n",
    "actions = [(-1, 0), (1, 0), (0, 1), (0, -1)]\n",
    "\n",
    "def transition(state, action):\n",
    "    new_state = (max(min(state[0] + action[0], row_size - 1), 0),\n",
    "                 max(min(state[1] + action[1], col_size - 1), 0))\n",
    "    return new_state if new_state != state else state\n",
    "\n",
    "def take_action(state, action):\n",
    "    next_state = transition(state, action)\n",
    "    reward = 20 if next_state == goal else -100 if next_state in cliff or next_state in snake_pit else -1\n",
    "    terminal = next_state == goal or next_state in cliff\n",
    "    return next_state, reward, terminal\n",
    "\n",
    "def choose_action(state, qvalues, epsilon):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return random.choice(actions)\n",
    "    else:\n",
    "        return max(actions, key=lambda a: qvalues[state][a])\n",
    "\n",
    "def show_route(visited_states):\n",
    "    for i in range(row_size):\n",
    "        print('-' * (col_size * 4))\n",
    "        for j in range(col_size):\n",
    "            token = 'R' if (i, j) in visited_states else 'G' if (i, j) == goal else '*' if (i, j) in cliff else '0'\n",
    "            print(f'| {token} ', end='')\n",
    "        print('|')\n",
    "    print('-' * (col_size * 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "03cbf20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def showRoute(states):\n",
    "    board = np.zeros([row_size, col_size])\n",
    "\n",
    "    # Mark cliff locations as -1\n",
    "    for c in cliff:\n",
    "        board[c[0], c[1]] = -1\n",
    "\n",
    "    # Mark snake pit locations as -2\n",
    "    for s in snake_pit:\n",
    "        board[s[0], s[1]] = -2\n",
    "\n",
    "    # Mark the route taken\n",
    "    for state in states:\n",
    "        if state != start and state != goal and board[state[0], state[1]] == 0:\n",
    "            board[state[0], state[1]] = 1\n",
    "\n",
    "    for i in range(row_size):\n",
    "        print('--------------------------------------------')\n",
    "        out = ''\n",
    "        for j in range(col_size):\n",
    "            token = ' '\n",
    "            if board[i, j] == -1:\n",
    "                token = 'C'  # Cliff\n",
    "            elif board[i, j] == -2:\n",
    "                token = 'O'  # Snake pit\n",
    "            elif board[i, j] == 1:\n",
    "                token = 'R'  # Route\n",
    "            if (i, j) == start:\n",
    "                token = 'S'  # Start\n",
    "            if (i, j) == goal:\n",
    "                token = 'G'  # Goal\n",
    "            out += token + ' '\n",
    "        print(out)\n",
    "    print('--------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "18f41451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(episodes, epsilon, start, alpha, qvalues=None):\n",
    "    if qvalues is None:\n",
    "        qvalues = {(i, j): {tuple(a): 0.0 for a in actions}  # Convert actions to tuples\n",
    "                   for i in range(row_size) for j in range(col_size)}\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        current_state = start\n",
    "        current_action = choose_action(current_state, qvalues, epsilon)\n",
    "        terminal = False\n",
    "\n",
    "        while not terminal:\n",
    "            next_state, reward, terminal = take_action(current_state, current_action)\n",
    "            next_action = choose_action(next_state, qvalues, epsilon) if not terminal else None\n",
    "\n",
    "            # SARSA update\n",
    "            current_value = qvalues[current_state][current_action]\n",
    "            next_value = qvalues[next_state][next_action] if next_action is not None else 0\n",
    "            qvalues[current_state][current_action] += alpha * (reward + gamma * next_value - current_value)\n",
    "\n",
    "            current_state, current_action = next_state, next_action\n",
    "\n",
    "    return qvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3f3c979d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qlearning(episodes, epsilon, start, alpha, gamma, qvalues=None):\n",
    "    if qvalues is None:\n",
    "        qvalues = {(i, j): {a: 0.0 for a in actions}\n",
    "                   for i in range(row_size) for j in range(col_size)}\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        current_state = start\n",
    "        terminal = False\n",
    "\n",
    "        while not terminal:\n",
    "            current_action = choose_action(current_state, qvalues, epsilon)\n",
    "            next_state, reward, terminal = take_action(current_state, current_action)\n",
    "\n",
    "            # Q-Learning update\n",
    "            current_value = qvalues[current_state][current_action]\n",
    "            next_value = max(qvalues[next_state].values()) if not terminal else 0\n",
    "            qvalues[current_state][current_action] += alpha * (reward + gamma * next_value - current_value)\n",
    "\n",
    "            current_state = next_state\n",
    "\n",
    "    return qvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fe89998e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_policy_path(start, qvalues):\n",
    "    current_state = start\n",
    "    policy_path = [current_state]\n",
    "    terminal = False\n",
    "\n",
    "    while not terminal:\n",
    "        current_action = max(qvalues[current_state], key=qvalues[current_state].get)\n",
    "        next_state, _, terminal = take_action(current_state, current_action)\n",
    "        policy_path.append(next_state)\n",
    "        current_state = next_state\n",
    "        if current_state == goal:  \n",
    "            break\n",
    "\n",
    "    return policy_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "22f615fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 0), (2, 0), (1, 0), (1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (0, 5), (0, 6), (0, 7), (0, 8), (0, 9), (0, 10), (0, 11), (0, 12), (0, 13), (0, 14), (1, 14), (1, 15), (1, 16), (1, 17), (1, 18), (1, 19), (1, 20), (2, 20), (3, 20)]\n",
      "--------------------------------------------\n",
      "          R R R R R R R R R R             \n",
      "--------------------------------------------\n",
      "R R R R R R         O       R R R R R R R \n",
      "--------------------------------------------\n",
      "R                   O                   R \n",
      "--------------------------------------------\n",
      "S C C C C C C C C C C C C C C C C C C C G \n",
      "--------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Run SARSA to get Q-values\n",
    "qs_s = sarsa(1000, epsilon, start, alpha)\n",
    "\n",
    "# Generate policy path from the learned Q-values\n",
    "states_s = generate_policy_path(start, qs_s)\n",
    "\n",
    "print(states_s)\n",
    "showRoute(states_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "380798df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 0), (2, 0), (2, 1), (2, 2), (2, 3), (1, 3), (1, 4), (1, 5), (1, 6), (0, 6), (0, 7), (0, 8), (0, 9), (0, 10), (0, 11), (0, 12), (0, 13), (0, 14), (1, 14), (1, 15), (1, 16), (2, 16), (2, 17), (2, 18), (2, 19), (2, 20), (3, 20)]\n",
      "--------------------------------------------\n",
      "            R R R R R R R R R             \n",
      "--------------------------------------------\n",
      "      R R R R       O       R R R         \n",
      "--------------------------------------------\n",
      "R R R R             O           R R R R R \n",
      "--------------------------------------------\n",
      "S C C C C C C C C C C C C C C C C C C C G \n",
      "--------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Run Q-learning to get Q-values\n",
    "qs_q = qlearning(1000, epsilon, start, alpha, gamma)\n",
    "\n",
    "# Generate policy path from the learned Q-values\n",
    "states_q = generate_policy_path(start, qs_q)\n",
    "\n",
    "print(states_q)\n",
    "showRoute(states_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adac02c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
